{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbhaskar1/ML-Coursera/blob/master/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3puDYrJ_H4j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "352c1825-f385-4e76-bf2d-0c03a19a4370"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbvlJW8t_fXd",
        "colab_type": "text"
      },
      "source": [
        "Import the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQuW96uu_lAk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "dde9ce19-081e-4488-beae-b532502e1e4d"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "print(f'Image Shape: {X_train[0].shape}')\n",
        "print(f'Training Set Size: {X_train.shape[0]}')\n",
        "print(f'Testing Set Size: {X_test.shape[0]}')\n",
        "print(f'y Element Shape: {y_train[0].shape}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Image Shape: (28, 28)\n",
            "Training Set Size: 60000\n",
            "Testing Set Size: 10000\n",
            "y Element Shape: (10,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9ltUbVnFXeK",
        "colab_type": "text"
      },
      "source": [
        "Define some general use functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnGluvI6FZ30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "def cross_entropy(y, y_hat):\n",
        "  return -(np.matmul(y.T, np.log(np.maximum(y_hat, 0.0001))) + np.matmul((1-y).T, np.log(np.maximum(1-y_hat, 0.0001))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RcjPwtbFw0N",
        "colab_type": "text"
      },
      "source": [
        "Create Neural Network class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFiH9mdJAc5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, input_nodes, hidden_nodes, output_nodes):\n",
        "    self.input_nodes = input_nodes\n",
        "    self.hidden_nodes = hidden_nodes\n",
        "    self.output_nodes = output_nodes\n",
        "    \n",
        "    self.theta_1 = np.random.uniform(low=-1, high=1, size=(hidden_nodes, input_nodes + 1))\n",
        "    self.theta_2 = np.random.uniform(low=-1, high=1, size=(output_nodes, hidden_nodes + 1))\n",
        "  \n",
        "  def feed_forward(self, image, return_layers=False):  # Input will be a 28x28 image\n",
        "    x = image.reshape((self.input_nodes, 1))\n",
        "    x = np.append([[1]], x, axis=0)  # Add bias\n",
        "    \n",
        "    h = sigmoid(np.matmul(self.theta_1, x))\n",
        "    h = np.append([[1]], h, axis=0)  # Add bias\n",
        "    \n",
        "    o = sigmoid(np.matmul(self.theta_2, h))\n",
        "    return o if not return_layers else (x, h, o)\n",
        "  \n",
        "  def cost(self, X, y):  # J(theta)\n",
        "    J = 0\n",
        "    m = X.shape[0]\n",
        "    for i in range(m):\n",
        "      J += cross_entropy(y[i], self.feed_forward(X[i]))\n",
        "    return J/m\n",
        "  \n",
        "  def back_prop(self, X, y, lr=0.01):\n",
        "    Delta_1 = np.zeros(shape=self.theta_1.shape)\n",
        "    Delta_2 = np.zeros(shape=self.theta_2.shape)\n",
        "    m = X.shape[0]\n",
        "    for i in range(m):\n",
        "      a_1, a_2, a_3 = self.feed_forward(X[i], return_layers=True)\n",
        "      delta_3 = a_3 - y[i].reshape(self.output_nodes, 1)\n",
        "      delta_2 = np.matmul(self.theta_2.T, delta_3) * a_2 * (1-a_2)\n",
        "      \n",
        "      Delta_2 += np.matmul(delta_3, a_2.T)\n",
        "      Delta_1 += np.matmul(delta_2[1:], a_1.T)\n",
        "    Delta_1 /= m\n",
        "    Delta_2 /= m\n",
        "    self.theta_1 -= lr * Delta_1\n",
        "    self.theta_2 -= lr * Delta_2\n",
        "  \n",
        "  def accuracy(self, X, y):\n",
        "    correct = 0\n",
        "    m = X.shape[0]\n",
        "    for i in range(m):\n",
        "      if np.argmax(self.feed_forward(X[i])) == np.argmax(y[i]):\n",
        "        correct += 1\n",
        "    return correct/m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqLV-VD3ALWS",
        "colab_type": "text"
      },
      "source": [
        "Setup Neural Network (784 node (+1 bias) input layer, 200 node (+1 bias) hidden layer, and 10 node output layer) and start learning process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN9AZVxi9AvI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "98130e1d-2e2a-4d96-e318-73f260a20a7a"
      },
      "source": [
        "network = NeuralNetwork(784, 200, 10)\n",
        "\n",
        "NUM_ITERATIONS = 200\n",
        "PRINT_EVERY = 10\n",
        "BATCH_SIZE = 1000\n",
        "\n",
        "for i in range(1, NUM_ITERATIONS + 1):\n",
        "  batch_index = ((i-1) * BATCH_SIZE) % X_train.shape[0]\n",
        "  network.back_prop(X_train[batch_index:batch_index + BATCH_SIZE], y_train[batch_index:batch_index + BATCH_SIZE], lr=0.1)\n",
        "  if i % PRINT_EVERY == 0:\n",
        "    print(f'Iteration {i}: Training Accuracy = {network.accuracy(X_train, y_train)}, Testing Accuracy = {network.accuracy(X_test, y_test)}')\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 10: Training Accuracy = 0.15886666666666666, Testing Accuracy = 0.1622\n",
            "Iteration 20: Training Accuracy = 0.24608333333333332, Testing Accuracy = 0.2456\n",
            "Iteration 30: Training Accuracy = 0.3245, Testing Accuracy = 0.3236\n",
            "Iteration 40: Training Accuracy = 0.38826666666666665, Testing Accuracy = 0.3852\n",
            "Iteration 50: Training Accuracy = 0.4421, Testing Accuracy = 0.4429\n",
            "Iteration 60: Training Accuracy = 0.4794333333333333, Testing Accuracy = 0.4837\n",
            "Iteration 70: Training Accuracy = 0.51595, Testing Accuracy = 0.5223\n",
            "Iteration 80: Training Accuracy = 0.5425166666666666, Testing Accuracy = 0.5446\n",
            "Iteration 90: Training Accuracy = 0.5651166666666667, Testing Accuracy = 0.57\n",
            "Iteration 100: Training Accuracy = 0.58685, Testing Accuracy = 0.589\n",
            "Iteration 110: Training Accuracy = 0.60315, Testing Accuracy = 0.6021\n",
            "Iteration 120: Training Accuracy = 0.6209166666666667, Testing Accuracy = 0.6219\n",
            "Iteration 130: Training Accuracy = 0.6336833333333334, Testing Accuracy = 0.6412\n",
            "Iteration 140: Training Accuracy = 0.6475, Testing Accuracy = 0.6533\n",
            "Iteration 150: Training Accuracy = 0.6603833333333333, Testing Accuracy = 0.6633\n",
            "Iteration 160: Training Accuracy = 0.6712833333333333, Testing Accuracy = 0.6734\n",
            "Iteration 170: Training Accuracy = 0.6799333333333333, Testing Accuracy = 0.6852\n",
            "Iteration 180: Training Accuracy = 0.6902333333333334, Testing Accuracy = 0.6921\n",
            "Iteration 190: Training Accuracy = 0.69875, Testing Accuracy = 0.7002\n",
            "Iteration 200: Training Accuracy = 0.7078666666666666, Testing Accuracy = 0.7081\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}